# Spark Pipeline

### by Jess Schueler

#### *Using Apache Spark to build an ETL pipeline*

## Technologies Used
* Python
* PySpark
* Pandas
* Jupyter Notebook

## Description 

Intakes a CSV file, adds columns to the dataframe, performs computations to better understand the data, then writes the data to a parquet file and displays a chart of thhe daily change in price averaged out over the years represented in our data set.

## Setup/Installation Requirements
* In the terminal, clone github repository using the following command;
    ```
    $ git clone https://github.com/jessgschueler/Spark-Pipeline
    ```
* In a venv, Pip install requirements.txt file
* Create a /data directory and run get_data.sh inside it
* Run the main.py file

## Known Bugs
* None at this time

## License
MIT

Copyright (c) 7/1/22 Jess Schueler
